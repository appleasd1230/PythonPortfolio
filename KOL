#coding:utf-8
from bs4 import BeautifulSoup as bs4
# from selenium import webdriver
import requests
import time as t
# import pandas as pd
import random
# import urllib3

"""取得各頁網址(抓出每頁的連結)"""
def get_url():
    global url_lst # 呼叫全域變數url_lst
    global result_count # 作者篇數
    page = 1
    state = True
    
    # 透過迴圈自動判斷哪幾頁是有資料的，再將其連結儲存起來
    while state:
        t.sleep(random.randint(10,15))
        Pubmed = '(hyperlipidemia[MeSH Terms] OR \
        hyperlipidemia OR hypercholesterolemia OR \
        hyperlipoproteinemia OR pcsk9) AND \
        ("2014/12/01"[Date - Publication] : "3000"[Date - Publication]) AND \
        (Wang YC[Author] AND Asia University Hospital)'
        Pubmed = Pubmed.replace(' ','%20').replace('[','%5B').replace(']','%5D').replace('"','%22').replace('Date - Publication','PDAT').replace('/','%2F')
        # url = 'https://www.ncbi.nlm.nih.gov/pubmed?term=' + \
        # '(hyperlipidemia%5BMeSH%20Terms%5D%20OR%20hyperlipidemia%5BAll%20F \
        # ields%5D%20OR%20hypercholesterolemia%5BAll%20Fields%5D%20OR%20hyperlipoproteinemia%5BAll%20Fields%5D%20OR%20pcsk9%5BAll%20F \
        # ields%5D)%20AND%20(%222014%2F12%2F01%22%5BPDAT%5D%20%3A%20%223000%22%5BPDAT%5D)%20AND%20(Wang%20YC%5BAuthor%5D%20AND%20Asia%20University%20Hospital)'
        url = 'https://www.ncbi.nlm.nih.gov/pubmed?term=' + Pubmed
        r = requests.get(url)
        # 判斷當前頁面是否為最後一頁
        soup = bs4(r.text, 'html.parser')
        item = soup.find_all(class_ = 'rprt')
        result_count = int(soup.find(class_ = 'result_count left').text[-1])


        print(result_count)
        for i in item:
            print(i.find('a').get('href'))
        break




"""取得頁面資訊"""
def get_content():
    global url_lst # 呼叫全域變數url_lst
    global csv_lst # 呼叫全域變數csv_lst
    driver = webdriver.Chrome('./chromedriver.exe') # 使用selenium中chrome的操作
    for url in url_lst:
        time_lst = [] # 各則新聞時間清單        
        title_lst = [] # 各則新聞標題清單
        href_lst = [] # 各則新聞連結清單
        popularity_lst = [] # 各則新聞人氣清單

        # 使用selenium模擬真實瀏覽器操作        
        r = driver.get(url)
        source = driver.page_source
        soup = bs4(source, 'html.parser') # 解析Html
        news = soup.find(class_ = 'forumgrid') # 抓取涵蓋所需內容範圍的Html Tag
        
        for tr in news.findAll('tr')[1:]:
            time_lst.append(str.strip(tr.findAll('td')[0].text))
            title_lst.append(tr.findAll('td')[1].text)
            popularity_lst.append(tr.findAll('td')[2].find('span').text) # selenium
            href = 'https://www.moneydj.com/' + tr.findAll('td')[1].find('a').get('href')
            href_lst.append(href)

        # 透過迴圈將資料存取起來
        for i in range(0, len(href_lst)-1): 
            csv_lst.append([time_lst[i], title_lst[i], popularity_lst[i], href_lst[i]])
    driver.close() # 關閉網頁
    
"""設定CSV的欄位標題"""
def writePandas(data_lst):
    df = pd.DataFrame(data=csv_lst, columns=['時間','主題','人氣','連結'])
    return df

"""將存取的資料轉成CSV格式輸出"""
def To_csv():
    global csv_lst
    fileName = 'moneydj' + t.strftime('%y%m%d', t.localtime())
    try:
        df = pd.read_csv('data/csv/' + fileName + '.csv')
        record = df['時間'].values.tolist()
        df = df.append(writePandas(csv_lst), ignore_index=False)
        df.to_csv('data/csv/'+fileName+'.csv', sep=',', encoding='utf_8_sig', index=False)
    except:
        with open('data/csv/'+fileName+'.csv', 'w') as new_csv:
            pass
        df = writePandas(csv_lst)
        df.to_csv(r'data/csv/'+fileName+'.csv', sep=',', encoding='utf_8_sig', index=False)

if __name__ == '__main__':
    # 設定全域變數
    url_lst = [] # 理財網-全頁網址
    csv_lst = [] # csv內容
    result_count = 0
    # 執行函式
    get_first_author()
    get_url() 
    # get_content()
    # To_csv()
